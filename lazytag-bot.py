import requests
import time
import json
import sys
import grab

from clarifai import rest
from clarifai.rest import ClarifaiApp

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn

import inflect
import markovify

app = ClarifaiApp(api_key='1f2c93de75074a088597cd7791491b5a')
model = app.public_models.general_model

## Lazytag Bot ##
# 1. Generate list of keywords from Clarifai model
# 2. Filter the keywords
# 3. ???
# 4. Generate comment

## Inputs ##
# an image url
sample_img = "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT_UtMjzw6-oCeHqiaiWyHVJMcv5a3Gz1UQ9DKR4t49PDKN6aQy"

# list of banned words
banned_words = {"nude"}

# list of subreddits for training data
subreddits = ["aww", "pics", "funny", "gaming", "AskReddit", "science", "worldnews",
"todayilearned", "videos", "movies", "Music", "IAmA", "gifs", "news", "EarthPorn",
"blog", "askscience", "Showerthoughts", "foodporn", "sports"]


'''
Clean the keyword list.
input: clarifai_keywords, banned_words
output: list of keywords, their lemmas, synonyms, and pluralized lemmas
*no duplicates
'''
def filter_keywords(clarifai_keywords, banned_words):
    lemmatizer = WordNetLemmatizer()
    keywords = set()
    for word in clarifai_keywords:
        # set of forms to check for duplicates
        forms = set()
        forms.add(word)

        # find lemma of input word
        lemma = lemmatizer.lemmatize(word)
        forms.add(lemma)

        # find derivational forms
        for lemma_type in wn.lemmas(lemma): # loop though the different definitions of the lemma
            for lemma_form in lemma_type.derivationally_related_forms():
                if lemma_form.name() not in forms:
                    forms.add(lemma_form.name())

        # add pluralized forms
        engine = inflect.engine()
        plural = engine.plural(lemma)
        forms.add(plural)
        
        # sanity check
        print(word, ": ", forms)

        # add if the form is not a banned word
        for word_form in forms:
            if word_form not in banned_words:
                keywords.add(word_form)

    # sanity check
    print ("final keywords:", keywords)

    return keywords

'''
Please put info here
'''
def generate_corpus(input_keywords, subreddits):
    # a part for getting from worddit api (what api?)
    # a part for matching keywords
    # a part for using reddit api

    keywords = {}
    for sr in subreddits:
        r = requests.get("http://www.keyworddit.com/api/retrieve/" + sr)
        data = r.json()

        sr_keywords = []
        for key in data:
            for word in key.split():
                sr_keywords.append(word)
        keywords[sr] = sr_keywords

    weights = {}
    for key in keywords:
        weights[key] = 0
    for label in input_keywords:
        for sr in keywords:
            for word in keywords[sr]:
                if word == label: weights[sr] += 1
    print(weights)
    subreddit_max = max(weights, key=weights.get)
    num_comments = 100
    corpus = grab.get_comments(subreddit_max, num_comments)
    return corpus

'''
Generates a set of sentences from a corpus
input: corpus of reddit comments
output: set of non-empty sentences.
'''
def generate_comment(model):
    # Train Model
    model = markovify.Text(corpus)

    # Generate Sentences
    for i in range(20):
        sentence = model.make_sentence()
        if (sentence != None):
            print("------------------------------")
            print(sentence)


# 1. Generate keywords from Clarifai model
# input: image url, sample_img
# output: keywords list, clarifai_keywords
print("##### Step 1. GENERATE KEYWORDS ######")
response = model.predict_by_url(sample_img)
clarifai_keywords = []
for dict_item in response['outputs'][0]['data']['concepts']:
    clarifai_keywords.append(dict_item['name'])

# Sanity check: print keywords generated by Clarifai
for item in clarifai_keywords:
    print(item)

# 2. Filter the keywords:
#   remove inappropriate words
#   lemmatize keywords and add the morphological versions
print("##### Step 2. CLEAN KEYWORDS ######")
keywords = filter_keywords(clarifai_keywords, banned_words)


# 3. Generate the training data for Markov Model.
print("##### Step 3. GENERATE TRAINING CORPUS ######")
corpus = generate_corpus(keywords, subreddits)

# 4. Generate the comment.
#   Train Markov model specific to the image
#   Create a bunch of sentences
print("##### Step 4. MARKOV MODEL ######")
generate_comment(model)


'''
def get_comments_from_reddit_api(comment_ids,author):
    headers = {'User-agent':'Comment Collector for /u/{}'.format(author)}
    params = {}
    params['id'] = ','.join(["t1_" + id for id in comment_ids])
    r = requests.get("https://api.reddit.com/api/info",params=params,headers=headers)
    data = r.json()
    return data['data']['children']
'''








